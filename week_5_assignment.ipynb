{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahiiru/idp-bootcamp/blob/main/week_5_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers datasets"
      ],
      "metadata": {
        "id": "7UyHioHsgfRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOtCaMtIg9_k",
        "outputId": "6c688e43-f7e6-4829-d38f-d444369d6fa4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation for traning"
      ],
      "metadata": {
        "id": "y8WGS7Oalvzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, tokenized_dataset):\n",
        "        self.tokenized_dataset = tokenized_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.tokenized_dataset[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(item['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(item['attention_mask'], dtype=torch.long),\n",
        "            'labels': torch.tensor(item['labels'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def prepare_data():\n",
        "    # Load the CoNLL-2003 dataset\n",
        "    dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "    # Get label list from dataset\n",
        "    label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "    def tokenize_and_align_labels(examples):\n",
        "        tokenized_inputs = tokenizer(\n",
        "            examples[\"tokens\"],\n",
        "            truncation=True,\n",
        "            is_split_into_words=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        labels = []\n",
        "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "            previous_word_idx = None\n",
        "            label_ids = []\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100)\n",
        "                elif word_idx != previous_word_idx:\n",
        "                    label_ids.append(label[word_idx])\n",
        "                else:\n",
        "                    label_ids.append(-100)\n",
        "                previous_word_idx = word_idx\n",
        "            labels.append(label_ids)\n",
        "\n",
        "        tokenized_inputs[\"labels\"] = labels\n",
        "        return tokenized_inputs\n",
        "\n",
        "    # Tokenize datasets\n",
        "    tokenized_datasets = dataset.map(\n",
        "        tokenize_and_align_labels,\n",
        "        batched=True,\n",
        "        remove_columns=dataset[\"train\"].column_names\n",
        "    )\n",
        "\n",
        "    return tokenized_datasets, len(label_list)\n"
      ],
      "metadata": {
        "id": "Eeykd69ggUzu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the model"
      ],
      "metadata": {
        "id": "rFxVJfP3xOL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification\n",
        "import torch.nn as nn\n",
        "\n",
        "class NERModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = BertForTokenClassification.from_pretrained(\n",
        "            'bert-base-cased',\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "I_FW_GkigvuZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the traning loop"
      ],
      "metadata": {
        "id": "fANj46xDyKdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kZslVJZ-oUS",
        "outputId": "8456c82b-b0ad-4bf7-b925-bcbe286aaf7b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, device, num_epochs=3):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=True)\n",
        "        for sample_index, sample in enumerate(progress_bar):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                sample['input_ids'].to(device),\n",
        "                sample['attention_mask'].to(device),\n",
        "                sample['labels'].to(device)\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {total_loss / len(train_dataloader)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CkmsKEJ3x3nB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the evaluation function"
      ],
      "metadata": {
        "id": "024hkY9gydgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcfEsdLgzZx7",
        "outputId": "6d677b2a-6962-40e0-edf9-50197c16d692"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "def evaluate_model(model, test_dataloader, id2label, device):\n",
        "    model.eval()\n",
        "    # Your code here\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Convert logits to predictions\n",
        "            predictions = torch.argmax(logits, dim=2)\n",
        "            predictions = predictions.cpu().numpy()\n",
        "            labels = labels.cpu().numpy()\n",
        "\n",
        "            # Convert predictions and labels to label names\n",
        "            for prediction, label in zip(predictions, labels):\n",
        "                true_label_names = [id2label[l] for l in label if l != -100]\n",
        "                predicted_label_names = [id2label[p] for p, l in zip(prediction, label) if l != -100]\n",
        "                true_labels.append(true_label_names)\n",
        "                predicted_labels.append(predicted_label_names)\n",
        "\n",
        "    # Print classification report\n",
        "    print(classification_report(true_labels, predicted_labels))"
      ],
      "metadata": {
        "id": "RchFco08yky1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the main() function"
      ],
      "metadata": {
        "id": "QFeiqJbi2k_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def main():\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load and prepare dataset\n",
        "    print(\"Loading and preparing dataset...\")\n",
        "    tokenized_datasets, num_labels = prepare_data()\n",
        "\n",
        "    # Convert to custom Dataset objects\n",
        "    train_dataset = NERDataset(tokenized_datasets[\"train\"])\n",
        "    eval_dataset = NERDataset(tokenized_datasets[\"validation\"])\n",
        "    test_dataset = NERDataset(tokenized_datasets[\"test\"])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=256,  # Increased batch size\n",
        "        shuffle=True\n",
        "    )\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=256\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=256\n",
        "    )\n",
        "\n",
        "    # Model initialization\n",
        "    model = NERModel(num_labels=num_labels)\n",
        "    model.to(device)\n",
        "\n",
        "    # Define optimizer with better parameters\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-5,  # Increased learning rate\n",
        "        weight_decay=0.01,\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    num_epochs = 3\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    num_warmup_steps = num_training_steps // 10\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"Starting training...\")\n",
        "    train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
        "    # save trained model\n",
        "    torch.save(model, \"bert_ner_model.pth\")\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nEvaluating on validation set...\")\n",
        "    id2label = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\",\n",
        "        2: \"I-PER\",\n",
        "        3: \"B-ORG\",\n",
        "        4: \"I-ORG\",\n",
        "        5: \"B-LOC\",\n",
        "        6: \"I-LOC\",\n",
        "        7: \"B-MISC\",\n",
        "        8: \"I-MISC\"\n",
        "    }\n",
        "\n",
        "    evaluate_model(model, eval_dataloader, id2label, device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JQ5EYOtRzXK9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcCdZiTJ2jYS",
        "outputId": "94f2d6be-1a22-4eb2-d56a-5b7ec5b1b915"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading and preparing dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|██████████| 55/55 [01:02<00:00,  1.14s/it, loss=0.122]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Average Loss: 0.705366049842401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 55/55 [01:01<00:00,  1.12s/it, loss=0.0494]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Average Loss: 0.08070390102538195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 55/55 [01:01<00:00,  1.12s/it, loss=0.0441]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Average Loss: 0.047781018303199245\n",
            "\n",
            "Evaluating on validation set...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.94      0.94      0.94      1837\n",
            "        MISC       0.80      0.82      0.81       922\n",
            "         ORG       0.88      0.91      0.89      1341\n",
            "         PER       0.98      0.97      0.97      1836\n",
            "\n",
            "   micro avg       0.91      0.92      0.92      5936\n",
            "   macro avg       0.90      0.91      0.90      5936\n",
            "weighted avg       0.91      0.92      0.92      5936\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement inference function\n"
      ],
      "metadata": {
        "id": "gEN8k-e1CxgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def prepare_inference(model_path=None):\n",
        "    \"\"\"Initialize tokenizer and load model for inference\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "    # Load trained model if path provided, otherwise use existing model\n",
        "    if model_path:\n",
        "        model = torch.load(model_path)\n",
        "\n",
        "    id2label = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\",\n",
        "        2: \"I-PER\",\n",
        "        3: \"B-ORG\",\n",
        "        4: \"I-ORG\",\n",
        "        5: \"B-LOC\",\n",
        "        6: \"I-LOC\",\n",
        "        7: \"B-MISC\",\n",
        "        8: \"I-MISC\"\n",
        "    }\n",
        "\n",
        "    return tokenizer, id2label\n",
        "\n",
        "def inference(text, model, tokenizer, id2label, device='cuda'):\n",
        "    \"\"\"\n",
        "    Perform NER inference on input text\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "        model: Trained NER model\n",
        "        tokenizer: BERT tokenizer\n",
        "        id2label (dict): Mapping from label ids to label names\n",
        "        device (str): Device to run inference on ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples containing (word, entity_label)\n",
        "    \"\"\"\n",
        "    ## fill in your code\n",
        "\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        is_split_into_words=False,\n",
        "        padding='max_length',\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Perform inference\n",
        "    outputs = model(\n",
        "        tokenized_inputs['input_ids'].to(device),\n",
        "        tokenized_inputs['attention_mask'].to(device)\n",
        "    )\n",
        "\n",
        "\n",
        "    # Convert predictions to labels\n",
        "    predicted_labels = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n",
        "    predicted_labels = [id2label[label] for label in predicted_labels]\n",
        "\n",
        "\n",
        "    # Align and merge subword tokens with labels\n",
        "    tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids'][0])\n",
        "    labeled_words = []\n",
        "    current_word = \"\"\n",
        "    current_label = None\n",
        "\n",
        "    for token, label in zip(tokens, predicted_labels):\n",
        "        if token == \"[PAD]\":\n",
        "            break\n",
        "        elif token.startswith(\"##\"):\n",
        "            current_word += token[2:]\n",
        "        else:\n",
        "            # If accumulated word is there, add it with its label\n",
        "            if current_word:\n",
        "                labeled_words.append((current_word, current_label))\n",
        "            # Start a new word\n",
        "            current_word = token\n",
        "            current_label = label\n",
        "\n",
        "    # Append the last accumulated word\n",
        "    if current_word:\n",
        "        labeled_words.append((current_word, current_label))\n",
        "\n",
        "    return labeled_words\n",
        "\n",
        "def print_entities(labeled_words):\n",
        "    \"\"\"Pretty print the labeled entities\"\"\"\n",
        "    current_entity = None\n",
        "    entity_text = []\n",
        "\n",
        "    for word, label in labeled_words:\n",
        "        if label == \"O\":\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = None\n",
        "                entity_text = []\n",
        "        elif label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "            current_entity = label[2:]  # Remove \"B-\" prefix\n",
        "            entity_text = [word]\n",
        "        elif label.startswith(\"I-\"):\n",
        "            if current_entity == label[2:]:  # If it's the same entity type\n",
        "                entity_text.append(word)\n",
        "            else:\n",
        "                if current_entity:\n",
        "                    print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = label[2:]\n",
        "                entity_text = [word]\n",
        "\n",
        "    if current_entity:  # Print last entity if exists\n",
        "        print(f\"{current_entity}: {' '.join(entity_text)}\")"
      ],
      "metadata": {
        "id": "0WZwPk5p22hn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First initialize\n",
        "tokenizer, id2label = prepare_inference()\n",
        "\n",
        "# Example texts to analyze\n",
        "texts = [\n",
        "    \"John Smith works at Microsoft in Seattle and visited New York last summer.\",\n",
        "    \"The European Union signed a trade deal with Japan in Brussels.\",\n",
        "    \"Tesla CEO Elon Musk announced new features coming to their vehicles.\"\n",
        "]\n",
        "\n",
        "# Process each text\n",
        "for text in texts:\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Entities found:\")\n",
        "    results = inference(text, model, tokenizer, id2label)\n",
        "    print_entities(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwooWAI3Cm1K",
        "outputId": "a3626e63-038d-4234-de79-f40ab173eaab"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: John Smith works at Microsoft in Seattle and visited New York last summer.\n",
            "Entities found:\n",
            "PER: John Smith\n",
            "ORG: Microsoft\n",
            "LOC: Seattle\n",
            "LOC: New York\n",
            "\n",
            "Text: The European Union signed a trade deal with Japan in Brussels.\n",
            "Entities found:\n",
            "ORG: European Union\n",
            "LOC: Japan\n",
            "LOC: Brussels\n",
            "\n",
            "Text: Tesla CEO Elon Musk announced new features coming to their vehicles.\n",
            "Entities found:\n",
            "ORG: Tesla\n",
            "PER: Elon Musk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}