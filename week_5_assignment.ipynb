{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lahiiru/idp-bootcamp/blob/main/week_5_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are Named Entities?\n",
        "\n",
        "Named entities are words or phrases that refer to specific persons, organizations, locations, dates, quantities, or other real-world objects or concepts.\n",
        "\n",
        "In the sentence:\n",
        "\n",
        "\"Apple CEO Tim Cook spoke in New York on Monday\"\n",
        "\n",
        "what are the named entities?"
      ],
      "metadata": {
        "id": "3QktUWTeK1t8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*\"Apple\" (organization), \"Tim Cook\" (person), \"New York\" (location), and \"Monday\" (time)*"
      ],
      "metadata": {
        "id": "X08zNARhL8nw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Techniques for Named Entity Extraction\n",
        "\n",
        "Can you name some techniques we can use for Named Entity extraction?\n",
        "\n",
        "Let's build a rule-based Named Entity extraction system. Here's your passage:\n",
        "\n",
        "\n",
        "> The Ministry of Education announced at 1 PM on 12th of October 2024 that Perera scored the highest in the Advanced Level examination at Royal College. Captain Cook Restaurant near Lake Gardens is owned by Mr. Silva, who used to teach at Trinity. While Elephant House ice cream remains popular, new brands like Happy Cow from Green Farms Ltd. are gaining market share. The Morning News reported that Dialog's CEO will speak at SLIIT next Friday about Smart Living solutions. The company's annual meeting is scheduled for 1300 10/12/2024 at Unity Plaza. Meanwhile, Unity Plaza shops are preparing for the April Season sales, which coincide with both New Year celebrations. Eagle Insurance (now merged with Union Bank) opened their branch opposite Victoria Park, where Peace Pagoda is clearly visible. Speaking of peace, Peace Cola is launching their new drink called Life, competing with Sprite and Sprint.\n",
        "\n",
        "Now write a function that extracts all\n",
        "-- persons\n",
        "-- organisations\n",
        "-- locations\n",
        "-- brands\n",
        "-- dates or time\n",
        "\n"
      ],
      "metadata": {
        "id": "cfAKbJPoMlxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = \"The Ministry of Education announced at 1 PM on 12th of October 2024 that Perera scored the highest in the Advanced Level examination at Royal College. Captain Cook Restaurant near Lake Gardens is owned by Mr. Silva, who used to teach at Trinity. While Elephant House ice cream remains popular, new brands like Happy Cow from Green Farms Ltd. are gaining market share. The Morning News reported that Dialog's CEO will speak at SLIIT next Friday about Smart Living solutions. The company's annual meeting is scheduled for 1300 10/12/2024 at Unity Plaza. Meanwhile, Unity Plaza shops are preparing for the April Season sales, which coincide with both New Year celebrations. Eagle Insurance (now merged with Union Bank) opened their branch opposite Victoria Park, where Peace Pagoda is clearly visible. Speaking of peace, Peace Cola is launching their new drink called Life, competing with Sprite and Sprint.\""
      ],
      "metadata": {
        "id": "yAhjmJwhPTXY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write dates using a regex which follows format 12th of October 2024\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_dates(text):\n",
        "    # Regular expression to match dates in the format \"12th of October 2024\"\n",
        "    date_pattern = r\"\\d{1,2}(st|nd|rd|th) of [A-Z][a-z]+ \\d{4}\"\n",
        "    dates = re.findall(date_pattern, text)\n",
        "    return dates\n",
        "\n",
        "# Example usage with the provided text\n",
        "p = \"The Ministry of Education announced at 1 PM on 12th of October 2024 that Perera scored the highest in the Advanced Level examination at Royal College. Captain Cook Restaurant near Lake Gardens is owned by Mr. Silva, who used to teach at Trinity. While Elephant House ice cream remains popular, new brands like Happy Cow from Green Farms Ltd. are gaining market share. The Morning News reported that Dialog's CEO will speak at SLIIT next Friday about Smart Living solutions. The company's annual meeting is scheduled for 1300 10/12/2024 at Unity Plaza. Meanwhile, Unity Plaza shops are preparing for the April Season sales, which coincide with both New Year celebrations. Eagle Insurance (now merged with Union Bank) opened their branch opposite Victoria Park, where Peace Pagoda is clearly visible. Speaking of peace, Peace Cola is launching their new drink called Life, competing with Sprite and Sprint.\"\n",
        "dates = extract_dates(p)\n",
        "dates\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA0uxXMTNgdn",
        "outputId": "0e3426d7-1d2d-42bd-e634-dbb6f9d29635"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['th']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Extraction with BERT\n",
        "\n",
        "## What is BERT?\n",
        "\n",
        "Read the paper: https://arxiv.org/pdf/1810.04805\n",
        "\n",
        "Read this blog to understand transformers: https://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "## CoNLL dataset\n",
        "\n",
        "Read more about it here: https://www.clips.uantwerpen.be/conll2003/ner/"
      ],
      "metadata": {
        "id": "hLm7LhbZQt0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UyHioHsgfRR",
        "outputId": "1b3ac30d-14fb-44f8-de4c-e25130ba03a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOtCaMtIg9_k",
        "outputId": "6c688e43-f7e6-4829-d38f-d444369d6fa4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's inspect the dataset\n",
        "sentence_0 = dataset[\"train\"][0]\n",
        "sentence_0"
      ],
      "metadata": {
        "id": "jZbACIbQg90Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117821f8-11b5-4701-db75-fefa73a0eaef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you identify each of the keys?\n",
        "\n",
        "Explain:\n",
        "- id\n",
        "- tokens\n",
        "- pos_tags\n",
        "- chunk_tags\n",
        "- ner_tags\n",
        "\n"
      ],
      "metadata": {
        "id": "baazFSErhecZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_0_str = \" \".join(dataset[\"train\"][0]['tokens'])\n",
        "sentence_0_str"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "juGBYZcshrMO",
        "outputId": "636a575e-a1f8-4f03-b563-f40c680384cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'EU rejects German call to boycott British lamb .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map pos tag numbers to their labels\n",
        "pos_tags = dataset[\"train\"].features[\"pos_tags\"].feature.names\n",
        "chunk_tags = dataset[\"train\"].features[\"chunk_tags\"].feature.names\n",
        "ner_tags = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "print(pos_tags)\n",
        "print(chunk_tags)\n",
        "print(ner_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12WJQLzbiA05",
        "outputId": "b34e3393-93cb-49d6-a864-244eab9d3649"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
            "['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP']\n",
            "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"POS Tags for sentence_0\")\n",
        "for i, pos_ in enumerate(sentence_0['pos_tags']):\n",
        "    print(f\"{pos_}  \\t- {pos_tags[pos_]} \\t- {sentence_0['tokens'][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtu51lx8jDCH",
        "outputId": "651f2f33-c3f8-4f03-aecf-463115728384"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags for sentence_0\n",
            "22  \t- NNP \t- EU\n",
            "42  \t- VBZ \t- rejects\n",
            "16  \t- JJ \t- German\n",
            "21  \t- NN \t- call\n",
            "35  \t- TO \t- to\n",
            "37  \t- VB \t- boycott\n",
            "16  \t- JJ \t- British\n",
            "21  \t- NN \t- lamb\n",
            "7  \t- . \t- .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chunk Tags for sentence_0\")\n",
        "for i, chunk_ in enumerate(sentence_0['chunk_tags']):\n",
        "    print(f\"{chunk_} \\t- {chunk_tags[chunk_]} \\t- {sentence_0['tokens'][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47xQckc8kIY7",
        "outputId": "cc726dd3-e063-4671-dbea-b68b93fab09c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk Tags for sentence_0\n",
            "11 \t- B-NP \t- EU\n",
            "21 \t- B-VP \t- rejects\n",
            "11 \t- B-NP \t- German\n",
            "12 \t- I-NP \t- call\n",
            "21 \t- B-VP \t- to\n",
            "22 \t- I-VP \t- boycott\n",
            "11 \t- B-NP \t- British\n",
            "12 \t- I-NP \t- lamb\n",
            "0 \t- O \t- .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NER Tags for sentence_0\")\n",
        "for i, ner_ in enumerate(sentence_0['ner_tags']):\n",
        "    print(f\"{ner_} \\t- {ner_tags[ner_]} \\t\\t- {sentence_0['tokens'][i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxBVp0VWkSiI",
        "outputId": "4e1df5a6-8620-4b11-f354-adbb44e38c73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER Tags for sentence_0\n",
            "3 \t- B-ORG \t\t- EU\n",
            "0 \t- O \t\t- rejects\n",
            "7 \t- B-MISC \t\t- German\n",
            "0 \t- O \t\t- call\n",
            "0 \t- O \t\t- to\n",
            "0 \t- O \t\t- boycott\n",
            "7 \t- B-MISC \t\t- British\n",
            "0 \t- O \t\t- lamb\n",
            "0 \t- O \t\t- .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the size of the dataset\n",
        "\n",
        "print(f\"Train size: {len(dataset['train'])}\")\n",
        "print(f\"Validation size: {len(dataset['validation'])}\")\n",
        "print(f\"Test size: {len(dataset['test'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg37XUTXlctD",
        "outputId": "60493eca-95c8-49fb-d971-dc5391fe2953"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 14041\n",
            "Validation size: 3250\n",
            "Test size: 3453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation for traning"
      ],
      "metadata": {
        "id": "y8WGS7Oalvzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, tokenized_dataset):\n",
        "        self.tokenized_dataset = tokenized_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.tokenized_dataset[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(item['input_ids'], dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(item['attention_mask'], dtype=torch.long),\n",
        "            'labels': torch.tensor(item['labels'], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def prepare_data():\n",
        "    # Load the CoNLL-2003 dataset\n",
        "    dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "    # Get label list from dataset\n",
        "    label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "    def tokenize_and_align_labels(examples):\n",
        "        tokenized_inputs = tokenizer(\n",
        "            examples[\"tokens\"],\n",
        "            truncation=True,\n",
        "            is_split_into_words=True,\n",
        "            padding='max_length',\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"  # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        labels = []\n",
        "        for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "            previous_word_idx = None\n",
        "            label_ids = []\n",
        "            for word_idx in word_ids:\n",
        "                if word_idx is None:\n",
        "                    label_ids.append(-100)\n",
        "                elif word_idx != previous_word_idx:\n",
        "                    label_ids.append(label[word_idx])\n",
        "                else:\n",
        "                    label_ids.append(-100)\n",
        "                previous_word_idx = word_idx\n",
        "            labels.append(label_ids)\n",
        "\n",
        "        tokenized_inputs[\"labels\"] = labels\n",
        "        return tokenized_inputs\n",
        "\n",
        "    # Tokenize datasets\n",
        "    tokenized_datasets = dataset.map(\n",
        "        tokenize_and_align_labels,\n",
        "        batched=True,\n",
        "        remove_columns=dataset[\"train\"].column_names\n",
        "    )\n",
        "\n",
        "    return tokenized_datasets, len(label_list)\n"
      ],
      "metadata": {
        "id": "Eeykd69ggUzu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the model"
      ],
      "metadata": {
        "id": "rFxVJfP3xOL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForTokenClassification\n",
        "import torch.nn as nn\n",
        "\n",
        "class NERModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = BertForTokenClassification.from_pretrained(\n",
        "            'bert-base-cased',\n",
        "            num_labels=num_labels\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "I_FW_GkigvuZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the traning loop"
      ],
      "metadata": {
        "id": "fANj46xDyKdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kZslVJZ-oUS",
        "outputId": "8456c82b-b0ad-4bf7-b925-bcbe286aaf7b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, device, num_epochs=3):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=True)\n",
        "        for sample_index, sample in enumerate(progress_bar):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                sample['input_ids'].to(device),\n",
        "                sample['attention_mask'].to(device),\n",
        "                sample['labels'].to(device)\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {total_loss / len(train_dataloader)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CkmsKEJ3x3nB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the evaluation function"
      ],
      "metadata": {
        "id": "024hkY9gydgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcfEsdLgzZx7",
        "outputId": "6d677b2a-6962-40e0-edf9-50197c16d692"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "def evaluate_model(model, test_dataloader, id2label, device):\n",
        "    model.eval()\n",
        "    # Your code here\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Convert logits to predictions\n",
        "            predictions = torch.argmax(logits, dim=2)\n",
        "            predictions = predictions.cpu().numpy()\n",
        "            labels = labels.cpu().numpy()\n",
        "\n",
        "            # Convert predictions and labels to label names\n",
        "            for prediction, label in zip(predictions, labels):\n",
        "                true_label_names = [id2label[l] for l in label if l != -100]\n",
        "                predicted_label_names = [id2label[p] for p, l in zip(prediction, label) if l != -100]\n",
        "                true_labels.append(true_label_names)\n",
        "                predicted_labels.append(predicted_label_names)\n",
        "\n",
        "    # Print classification report\n",
        "    print(classification_report(true_labels, predicted_labels))"
      ],
      "metadata": {
        "id": "RchFco08yky1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the main() function"
      ],
      "metadata": {
        "id": "QFeiqJbi2k_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def main():\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load and prepare dataset\n",
        "    print(\"Loading and preparing dataset...\")\n",
        "    tokenized_datasets, num_labels = prepare_data()\n",
        "\n",
        "    # Convert to custom Dataset objects\n",
        "    train_dataset = NERDataset(tokenized_datasets[\"train\"])\n",
        "    eval_dataset = NERDataset(tokenized_datasets[\"validation\"])\n",
        "    test_dataset = NERDataset(tokenized_datasets[\"test\"])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=256,  # Increased batch size\n",
        "        shuffle=True\n",
        "    )\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset,\n",
        "        batch_size=256\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=256\n",
        "    )\n",
        "\n",
        "    # Model initialization\n",
        "    model = NERModel(num_labels=num_labels)\n",
        "    model.to(device)\n",
        "\n",
        "    # Define optimizer with better parameters\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(),\n",
        "        lr=5e-5,  # Increased learning rate\n",
        "        weight_decay=0.01,\n",
        "        eps=1e-8\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    num_epochs = 3\n",
        "    num_training_steps = num_epochs * len(train_dataloader)\n",
        "    num_warmup_steps = num_training_steps // 10\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"Starting training...\")\n",
        "    train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, device, num_epochs=num_epochs)\n",
        "    # save trained model\n",
        "    torch.save(model, \"bert_ner_model.pth\")\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\nEvaluating on validation set...\")\n",
        "    id2label = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\",\n",
        "        2: \"I-PER\",\n",
        "        3: \"B-ORG\",\n",
        "        4: \"I-ORG\",\n",
        "        5: \"B-LOC\",\n",
        "        6: \"I-LOC\",\n",
        "        7: \"B-MISC\",\n",
        "        8: \"I-MISC\"\n",
        "    }\n",
        "\n",
        "    evaluate_model(model, eval_dataloader, id2label, device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JQ5EYOtRzXK9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcCdZiTJ2jYS",
        "outputId": "94f2d6be-1a22-4eb2-d56a-5b7ec5b1b915"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading and preparing dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|██████████| 55/55 [01:02<00:00,  1.14s/it, loss=0.122]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Average Loss: 0.705366049842401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 55/55 [01:01<00:00,  1.12s/it, loss=0.0494]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Average Loss: 0.08070390102538195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 55/55 [01:01<00:00,  1.12s/it, loss=0.0441]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Average Loss: 0.047781018303199245\n",
            "\n",
            "Evaluating on validation set...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.94      0.94      0.94      1837\n",
            "        MISC       0.80      0.82      0.81       922\n",
            "         ORG       0.88      0.91      0.89      1341\n",
            "         PER       0.98      0.97      0.97      1836\n",
            "\n",
            "   micro avg       0.91      0.92      0.92      5936\n",
            "   macro avg       0.90      0.91      0.90      5936\n",
            "weighted avg       0.91      0.92      0.92      5936\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement inference function\n"
      ],
      "metadata": {
        "id": "gEN8k-e1CxgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def prepare_inference(model_path=None):\n",
        "    \"\"\"Initialize tokenizer and load model for inference\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "    # Load trained model if path provided, otherwise use existing model\n",
        "    if model_path:\n",
        "        model = torch.load(model_path)\n",
        "\n",
        "    id2label = {\n",
        "        0: \"O\",\n",
        "        1: \"B-PER\",\n",
        "        2: \"I-PER\",\n",
        "        3: \"B-ORG\",\n",
        "        4: \"I-ORG\",\n",
        "        5: \"B-LOC\",\n",
        "        6: \"I-LOC\",\n",
        "        7: \"B-MISC\",\n",
        "        8: \"I-MISC\"\n",
        "    }\n",
        "\n",
        "    return tokenizer, id2label\n",
        "\n",
        "def inference(text, model, tokenizer, id2label, device='cuda'):\n",
        "    \"\"\"\n",
        "    Perform NER inference on input text\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "        model: Trained NER model\n",
        "        tokenizer: BERT tokenizer\n",
        "        id2label (dict): Mapping from label ids to label names\n",
        "        device (str): Device to run inference on ('cuda' or 'cpu')\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples containing (word, entity_label)\n",
        "    \"\"\"\n",
        "    ## fill in your code\n",
        "\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        truncation=True,\n",
        "        is_split_into_words=False,\n",
        "        padding='max_length',\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Perform inference\n",
        "    outputs = model(\n",
        "        tokenized_inputs['input_ids'].to(device),\n",
        "        tokenized_inputs['attention_mask'].to(device)\n",
        "    )\n",
        "\n",
        "\n",
        "    # Convert predictions to labels\n",
        "    predicted_labels = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n",
        "    predicted_labels = [id2label[label] for label in predicted_labels]\n",
        "\n",
        "\n",
        "    # Align and merge subword tokens with labels\n",
        "    tokens = tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids'][0])\n",
        "    labeled_words = []\n",
        "    current_word = \"\"\n",
        "    current_label = None\n",
        "\n",
        "    for token, label in zip(tokens, predicted_labels):\n",
        "        if token == \"[PAD]\":\n",
        "            break\n",
        "        elif token.startswith(\"##\"):\n",
        "            current_word += token[2:]\n",
        "        else:\n",
        "            # If accumulated word is there, add it with its label\n",
        "            if current_word:\n",
        "                labeled_words.append((current_word, current_label))\n",
        "            # Start a new word\n",
        "            current_word = token\n",
        "            current_label = label\n",
        "\n",
        "    # Append the last accumulated word\n",
        "    if current_word:\n",
        "        labeled_words.append((current_word, current_label))\n",
        "\n",
        "    return labeled_words\n",
        "\n",
        "def print_entities(labeled_words):\n",
        "    \"\"\"Pretty print the labeled entities\"\"\"\n",
        "    current_entity = None\n",
        "    entity_text = []\n",
        "\n",
        "    for word, label in labeled_words:\n",
        "        if label == \"O\":\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = None\n",
        "                entity_text = []\n",
        "        elif label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "            current_entity = label[2:]  # Remove \"B-\" prefix\n",
        "            entity_text = [word]\n",
        "        elif label.startswith(\"I-\"):\n",
        "            if current_entity == label[2:]:  # If it's the same entity type\n",
        "                entity_text.append(word)\n",
        "            else:\n",
        "                if current_entity:\n",
        "                    print(f\"{current_entity}: {' '.join(entity_text)}\")\n",
        "                current_entity = label[2:]\n",
        "                entity_text = [word]\n",
        "\n",
        "    if current_entity:  # Print last entity if exists\n",
        "        print(f\"{current_entity}: {' '.join(entity_text)}\")"
      ],
      "metadata": {
        "id": "0WZwPk5p22hn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First initialize\n",
        "tokenizer, id2label = prepare_inference()\n",
        "\n",
        "# Example texts to analyze\n",
        "texts = [\n",
        "    \"John Smith works at Microsoft in Seattle and visited New York last summer.\",\n",
        "    \"The European Union signed a trade deal with Japan in Brussels.\",\n",
        "    \"Tesla CEO Elon Musk announced new features coming to their vehicles.\"\n",
        "]\n",
        "\n",
        "# Process each text\n",
        "for text in texts:\n",
        "    print(\"\\nText:\", text)\n",
        "    print(\"Entities found:\")\n",
        "    results = inference(text, model, tokenizer, id2label)\n",
        "    print_entities(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwooWAI3Cm1K",
        "outputId": "a3626e63-038d-4234-de79-f40ab173eaab"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: John Smith works at Microsoft in Seattle and visited New York last summer.\n",
            "Entities found:\n",
            "PER: John Smith\n",
            "ORG: Microsoft\n",
            "LOC: Seattle\n",
            "LOC: New York\n",
            "\n",
            "Text: The European Union signed a trade deal with Japan in Brussels.\n",
            "Entities found:\n",
            "ORG: European Union\n",
            "LOC: Japan\n",
            "LOC: Brussels\n",
            "\n",
            "Text: Tesla CEO Elon Musk announced new features coming to their vehicles.\n",
            "Entities found:\n",
            "ORG: Tesla\n",
            "PER: Elon Musk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}